{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyShakespeare\n",
    "\n",
    "A character Level lanaguage model to generate shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "1115394\n"
     ]
    }
   ],
   "source": [
    "with open('tiny_shakespeare.txt', 'r') as file:\n",
    "    text=file.read()\n",
    "\n",
    "print(type(text))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is our Vocab of 65 characters: \n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Create Vocabulary (over characters)\n",
    "vocab=sorted(list(set(text)))\n",
    "print(f\"Here is our Vocab of {len(vocab)} characters: \\n{''.join(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Vocab Mappings\n",
    "\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to encode text strings and decode indicies\n",
    "\n",
    "def encode(text:str):\n",
    "    return [char_to_idx[char] for char in text]\n",
    "def decode(indices: list):\n",
    "    return ''.join([idx_to_char[idx] for idx in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# Encode entire dataset and store in a torch.tensor\n",
    "\n",
    "# 1D ARRAY\n",
    "data = torch.tensor(encode(text), dtype=torch.int64) ## by default it trys to interprete dtype --> best to just define\n",
    "print(data.shape)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1003854])\n",
      "torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "# split into train-val split - what about splitting over a word? well this is a character level model so doesnt matter!\n",
    "\n",
    "# Purpose: We dont want the model to just memorise the text. We want it to generate new, shakespeare like text.\n",
    "# So by with holding some data during training. We can evaluate how good our model is by how well it reproduces shakespeare its never seen.\n",
    "\n",
    "split = int(data.shape[0]*0.9) #90% Train 10% Validation\n",
    "train_data = data[:split]\n",
    "val_data = data[split:]\n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing the Dataset\n",
    "\n",
    "Two key points in the dataset:\n",
    "1. For a certain context length the data has multiple examples, its not just the full context length and the target. This aligns with point 2.\n",
    "2. We want the model to work with varying context lengths (up to the max context length) because the model may only be given a smaller context (ie. the text \"why?\" - which is 4 characters. If model only has fixed context length it wont be able to deal with this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [18] --> target: 47\n",
      "Context: [18, 47] --> target: 56\n",
      "Context: [18, 47, 56] --> target: 57\n",
      "Context: [18, 47, 56, 57] --> target: 58\n",
      "Context: [18, 47, 56, 57, 58] --> target: 1\n",
      "Context: [18, 47, 56, 57, 58, 1] --> target: 15\n",
      "Context: [18, 47, 56, 57, 58, 1, 15] --> target: 47\n",
      "Context: [18, 47, 56, 57, 58, 1, 15, 47] --> target: 58\n"
     ]
    }
   ],
   "source": [
    "context_len=8\n",
    "\n",
    "for i in range(1, len(train_data[:context_len])+1):\n",
    "    \n",
    "    x = train_data[:i]\n",
    "    y = train_data[i]\n",
    "    print(f\"Context: {x.tolist()} --> target: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating batches\n",
    "\n",
    "context_len=8\n",
    "batch_size=4\n",
    "\n",
    "# inputs and targets will be tensors of shape (4,8), where targets context will be shifted 1 place to the right.\n",
    "\n",
    "def create_batch(split:str):\n",
    "    data: torch.tensor = train_data if split==\"train\" else val_data\n",
    "    \n",
    "    # randomly select batch_size number of starting points in the data\n",
    "    # The upper bound is (len(data) - context_len) to ensure we can always build a complete\n",
    "    # context of size context_len, even if we start at the last valid position\n",
    "    starting_idxs: torch.tensor = torch.randint(0, len(data)-context_len, size=(batch_size,))\n",
    "    \n",
    "    #create a list of tensors, which we then stack\n",
    "    X=torch.stack([data[start:start+context_len] for start in starting_idxs])\n",
    "    y=torch.stack([data[start+1: start+context_len+1] for start in starting_idxs])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[44, 59, 52, 42, 39, 51, 43, 52],\n",
      "        [52, 43,  5, 43, 56,  1, 57, 53],\n",
      "        [ 0, 35, 43, 50, 50, 11,  1, 58],\n",
      "        [53, 58, 46, 43, 56,  6,  1, 40]])\n",
      "tensor([[59, 52, 42, 39, 51, 43, 52, 58],\n",
      "        [43,  5, 43, 56,  1, 57, 53,  1],\n",
      "        [35, 43, 50, 50, 11,  1, 58, 46],\n",
      "        [58, 46, 43, 56,  6,  1, 40, 59]])\n",
      "Total of 32 examples\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(196)\n",
    "\n",
    "X, y = create_batch(\"train\")\n",
    "print(X)\n",
    "print(y)\n",
    "print(f\"Total of {y.shape[0]*y.shape[1]} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Next Example in the Batch:\n",
      "\n",
      "Context: tensor([44]) --> Target: 59\n",
      "Context: tensor([44, 59]) --> Target: 52\n",
      "Context: tensor([44, 59, 52]) --> Target: 42\n",
      "Context: tensor([44, 59, 52, 42]) --> Target: 39\n",
      "Context: tensor([44, 59, 52, 42, 39]) --> Target: 51\n",
      "Context: tensor([44, 59, 52, 42, 39, 51]) --> Target: 43\n",
      "Context: tensor([44, 59, 52, 42, 39, 51, 43]) --> Target: 52\n",
      "Context: tensor([44, 59, 52, 42, 39, 51, 43, 52]) --> Target: 58\n",
      "\n",
      "Next Example in the Batch:\n",
      "\n",
      "Context: tensor([52]) --> Target: 43\n",
      "Context: tensor([52, 43]) --> Target: 5\n",
      "Context: tensor([52, 43,  5]) --> Target: 43\n",
      "Context: tensor([52, 43,  5, 43]) --> Target: 56\n",
      "Context: tensor([52, 43,  5, 43, 56]) --> Target: 1\n",
      "Context: tensor([52, 43,  5, 43, 56,  1]) --> Target: 57\n",
      "Context: tensor([52, 43,  5, 43, 56,  1, 57]) --> Target: 53\n",
      "Context: tensor([52, 43,  5, 43, 56,  1, 57, 53]) --> Target: 1\n",
      "\n",
      "Next Example in the Batch:\n",
      "\n",
      "Context: tensor([0]) --> Target: 35\n",
      "Context: tensor([ 0, 35]) --> Target: 43\n",
      "Context: tensor([ 0, 35, 43]) --> Target: 50\n",
      "Context: tensor([ 0, 35, 43, 50]) --> Target: 50\n",
      "Context: tensor([ 0, 35, 43, 50, 50]) --> Target: 11\n",
      "Context: tensor([ 0, 35, 43, 50, 50, 11]) --> Target: 1\n",
      "Context: tensor([ 0, 35, 43, 50, 50, 11,  1]) --> Target: 58\n",
      "Context: tensor([ 0, 35, 43, 50, 50, 11,  1, 58]) --> Target: 46\n",
      "\n",
      "Next Example in the Batch:\n",
      "\n",
      "Context: tensor([53]) --> Target: 58\n",
      "Context: tensor([53, 58]) --> Target: 46\n",
      "Context: tensor([53, 58, 46]) --> Target: 43\n",
      "Context: tensor([53, 58, 46, 43]) --> Target: 56\n",
      "Context: tensor([53, 58, 46, 43, 56]) --> Target: 6\n",
      "Context: tensor([53, 58, 46, 43, 56,  6]) --> Target: 1\n",
      "Context: tensor([53, 58, 46, 43, 56,  6,  1]) --> Target: 40\n",
      "Context: tensor([53, 58, 46, 43, 56,  6,  1, 40]) --> Target: 59\n"
     ]
    }
   ],
   "source": [
    "#Spelling out examples\n",
    "\n",
    "\n",
    "for batch in range(batch_size):\n",
    "    print(\"\\nNext Example in the Batch:\\n\")\n",
    "    for example_idx in range(context_len):\n",
    "        \n",
    "        context = X[batch][:example_idx+1] # Remember indexing is up to but not including e.ge [:0] is empty so [:0+1] is 1st example\n",
    "        target = y[batch][example_idx]\n",
    "        \n",
    "        print(f\"Context: {context} --> Target: {target}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Bigram Model\n",
    "\n",
    "As a baseline we will use the Bigram Language model. Which predicts the next character given a previous character. The prediction is made based on the frequency of bigram pairs (i.e. How many times does a come after r in our dataset). We collect all the pairwise combinations of our vocab (creating a (vocab_size, vocab_size) Look-up Table). We normalise over the row, to create a probability dist of the next character given a certain character. i.e. if the first character in our vocab is a. then the first row of our look-up table represents the frequency of every character in our vocab coming after a (i.e. given a). We normalise to create a probility distribution of the next character given a. These are the logits of the bigram model.\n",
    "\n",
    "On inference we can feed prob_dist into multinomial dist (which will sample the next character idx based on the prob_dist we gave). This idx is then used to index into our loo-up table, grabing the prob_dist of that character... and so on untill we reach an end character!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\"\n",
    "        Define Layers and Parameters\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bigram_lookup_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, idxs, targets=None):\n",
    "        \"\"\"\n",
    "        Define how data flows through the layers of the network\n",
    "        \n",
    "        idxss | (batch_dim, context_len): batches of indices of characters in our Vocab. \n",
    "        targets | (batch_dim, context_len): batch of target indices\n",
    "        \"\"\"\n",
    "        # For each index in idxs you'll have a 1D array of length vocab_size representing the \"frequency\" dist over the whole vocab given that index\n",
    "        logits = self.bigram_lookup_table(idxs) # (batch_dim, context_len, vocab_size) <-- (each row represents an index, along the columns is the dist for that index) we then have batch_dim num of those matricies\n",
    "        \n",
    "        if targets == None:\n",
    "            loss=None\n",
    "        else:\n",
    "            ## ----- Loss ----- ##\n",
    "            # ve- log likelihood <==> Cross entropy in this senario\n",
    "            # Cross Entropy wants the last dim to be num_classes (i.e. the len(vocab))\n",
    "            # reshape/view\n",
    "            batch_size, context_len, num_classes = logits.shape\n",
    "            logits = logits.view(batch_size*context_len, num_classes) # stack the batches. each row represents one example of context\n",
    "            targets = targets.view(batch_size*context_len) # long 1D array\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idxs, max_token_length):\n",
    "        \"\"\"\n",
    "        Define how the model goes about generating new tokens\n",
    "        \"\"\"\n",
    "        # No longer generating names. Now we want to, given a sequence (idxs) generate new tokens and concat to idxs.\n",
    "        \n",
    "        for i in range(max_token_length):\n",
    "            \n",
    "            # Calling self() or model() in Pytorch calls forward()\n",
    "            logits, _ = self(idxs, targets=None)\n",
    "            \n",
    "            # we only need the one context (as Bigram model). logits shape: (batch_dim, context_len, vocab_len)\n",
    "            # take the last context in each batch. So one row is taken from each batch giving shape (batch_dim, vocab_len)\n",
    "            logits = logits[:, -1, :] \n",
    "            \n",
    "            ## create prob_dists and sample ##\n",
    "            prob_dists = torch.softmax(logits, dim=1) # across the columns (so dim=1). Very Easy when you understand!\n",
    "            indicies = torch.multinomial(prob_dists, num_samples=1, replacement=True) # (batch_dim, 1) out of the vocab_len one is chosen\n",
    "            #append onto\n",
    "            \n",
    "            idxs = torch.cat((idxs, indicies), dim=1)\n",
    "        return idxs\n",
    "            \n",
    "             \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = BigramLanguageModel(vocab_size=len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[44, 59, 52, 42, 39, 51, 43, 52],\n",
       "        [52, 43,  5, 43, 56,  1, 57, 53],\n",
       "        [ 0, 35, 43, 50, 50, 11,  1, 58],\n",
       "        [53, 58, 46, 43, 56,  6,  1, 40]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5626,  0.9114,  2.2276,  ...,  1.0004, -1.9456,  0.6057],\n",
      "        [ 0.5789, -1.4890, -0.5426,  ...,  0.7079, -0.3116,  0.7640],\n",
      "        [-0.0932, -0.3452,  0.2444,  ...,  0.7291, -0.7309,  1.2668],\n",
      "        ...,\n",
      "        [ 0.0129,  1.2247,  0.7694,  ..., -0.1291,  0.1203, -1.4294],\n",
      "        [-0.7484, -1.6542,  0.1551,  ...,  0.2770,  1.2927, -0.1394],\n",
      "        [-0.3436,  2.7319, -1.6450,  ...,  1.0328, -0.8345, -1.7028]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor(4.5137, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Randomly initalised Look-up table (we havent done any training yet)\n",
    "# Select the very first row, which should act as a prob dist \n",
    "prob_dist, loss= bigram(idx=X, targets=y)\n",
    "print(prob_dist)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the loss to be -ln(1/65) ~= 4.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our input: \n",
      "tensor([[44, 59, 52, 42, 39, 51, 43, 52],\n",
      "        [52, 43,  5, 43, 56,  1, 57, 53],\n",
      "        [ 0, 35, 43, 50, 50, 11,  1, 58],\n",
      "        [53, 58, 46, 43, 56,  6,  1, 40]])\n",
      "Our output: \n",
      "tensor([[44, 59, 52, 42, 39, 51, 43, 52, 63, 61,  6, 19, 17, 35, 23, 61, 28, 11],\n",
      "        [52, 43,  5, 43, 56,  1, 57, 53, 11, 22, 25,  0, 19, 30, 55,  0, 46,  1],\n",
      "        [ 0, 35, 43, 50, 50, 11,  1, 58,  8, 41, 31,  8, 27, 64, 30, 57, 14, 24],\n",
      "        [53, 58, 46, 43, 56,  6,  1, 40, 24, 40, 19,  0, 31, 34, 58,  7, 30, 31]])\n"
     ]
    }
   ],
   "source": [
    "## Lets generate 10 new characters\n",
    "idx = bigram.generate(idx=X, max_token_length=10)\n",
    "print(f\"Our input: \\n{X}\")\n",
    "print(f\"Our output: \\n{idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cSOSGEaciPG&;awqO3hgOuYI:a!Wey-:qnJMJ'uYK;$HQR'aj'ci:SV&ciAUvN.PNHt'EuTO3haC'3y?aIH.VHnvNLriAczgSQu,\n"
     ]
    }
   ],
   "source": [
    "# starting with newline char as first token in the sequence, generating and then decoding\n",
    "idx = torch.zeros((1, 1), dtype=torch.int64)\n",
    "generation = bigram.generate(idx, max_token_length=100)\n",
    "print(decode(generation.flatten().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = BigramLanguageModel(vocab_size=len(vocab))\n",
    "optimiser = torch.optim.AdamW(bigram.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on 0th iteration: 3.4301\n",
      "Loss on 10000th iteration: 2.4263\n",
      "Loss on 20000th iteration: 2.3831\n",
      "Loss on 30000th iteration: 3.0552\n",
      "Loss on 40000th iteration: 3.2765\n",
      "Loss on 50000th iteration: 2.4752\n",
      "Loss on 60000th iteration: 2.5668\n",
      "Loss on 70000th iteration: 2.2361\n",
      "Loss on 80000th iteration: 2.3899\n",
      "Loss on 90000th iteration: 3.4464\n"
     ]
    }
   ],
   "source": [
    "num_steps=100_000\n",
    "for i in range(num_steps):\n",
    "    \n",
    "    #create a batch\n",
    "    X_batch, y_batch = create_batch(split=\"train\")\n",
    "    \n",
    "    logits, loss = bigram.forward(idx=X_batch, targets=y_batch)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "    if i % 10_000 == 0:\n",
    "        print(f\"Loss on {i}th iteration: {loss.item():.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BELAUS:\n",
      "\n",
      "A:\n",
      "\n",
      "BELI yis\n",
      "Noude atha, there, hie ongh, you woy,\n",
      "bumeal s pongbe t buprdd, onenos! a ungore a chads s, od.\n",
      "\n",
      "Asthad.\n",
      "Se that d S:\n",
      "Planke, ad ad, at:\n",
      "Ge\n",
      "TI liren, Makngrkillau itise bye ngr he toin; r t aved thy f l'le; me spro ad' d sonthyo\n"
     ]
    }
   ],
   "source": [
    "## Lets generate again...\n",
    "idx = torch.zeros((1, 1), dtype=torch.int64)\n",
    "generation = bigram.generate(idx, max_token_length=250)\n",
    "print(decode(generation.flatten().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
