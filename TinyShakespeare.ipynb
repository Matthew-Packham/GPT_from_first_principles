{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyShakespeare\n",
    "\n",
    "A character Level lanaguage model to generate shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "1115394\n"
     ]
    }
   ],
   "source": [
    "with open('tiny_shakespeare.txt', 'r') as file:\n",
    "    text=file.read()\n",
    "\n",
    "print(type(text))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is our Vocab of 65 characters: \n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Create Vocabulary (over characters)\n",
    "vocab=sorted(list(set(text)))\n",
    "print(f\"Here is our Vocab of {len(vocab)} characters: \\n{''.join(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Vocab Mappings\n",
    "\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to encode text strings and decode indicies\n",
    "\n",
    "def encode(text:str):\n",
    "    return [char_to_idx[char] for char in text]\n",
    "def decode(indices: list):\n",
    "    return ''.join([idx_to_char[idx] for idx in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# Encode entire dataset and store in a torch.tensor\n",
    "\n",
    "# 1D ARRAY\n",
    "data = torch.tensor(encode(text), dtype=torch.int64) ## by default it trys to interprete dtype --> best to just define\n",
    "print(data.shape)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1003854])\n",
      "torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "# split into train-val split - what about splitting over a word? well this is a character level model so doesnt matter!\n",
    "\n",
    "# Purpose: We dont want the model to just memorise the text. We want it to generate new, shakespeare like text.\n",
    "# So by with holding some data during training. We can evaluate how good our model is by how well it reproduces shakespeare its never seen.\n",
    "\n",
    "split = int(data.shape[0]*0.9) #90% Train 10% Validation\n",
    "train_data = data[:split]\n",
    "val_data = data[split:]\n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing the Dataset\n",
    "\n",
    "Two key points in the dataset:\n",
    "1. For a certain context length the data has multiple examples, its not just the full context length and the target. This aligns with point 2.\n",
    "2. We want the model to work with varying context lengths (up to the max context length) because the model may only be given a smaller context (ie. the text \"why?\" - which is 4 characters. If model only has fixed context length it wont be able to deal with this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [18] --> target: 47\n",
      "Context: [18, 47] --> target: 56\n",
      "Context: [18, 47, 56] --> target: 57\n",
      "Context: [18, 47, 56, 57] --> target: 58\n",
      "Context: [18, 47, 56, 57, 58] --> target: 1\n",
      "Context: [18, 47, 56, 57, 58, 1] --> target: 15\n",
      "Context: [18, 47, 56, 57, 58, 1, 15] --> target: 47\n",
      "Context: [18, 47, 56, 57, 58, 1, 15, 47] --> target: 58\n"
     ]
    }
   ],
   "source": [
    "context_len=8\n",
    "\n",
    "for i in range(1, len(train_data[:context_len])+1):\n",
    "    \n",
    "    x = train_data[:i]\n",
    "    y = train_data[i]\n",
    "    print(f\"Context: {x.tolist()} --> target: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating batches\n",
    "\n",
    "context_len=8\n",
    "batch_size=4\n",
    "\n",
    "# inputs and targets will be tensors of shape (4,8), where targets context will be shifted 1 place to the right.\n",
    "\n",
    "def create_batch(split:str):\n",
    "    data: torch.tensor = train_data if split==\"train\" else val_data\n",
    "    \n",
    "    # randomly select batch_size number of starting points in the data\n",
    "    # The upper bound is (len(data) - context_len) to ensure we can always build a complete\n",
    "    # context of size context_len, even if we start at the last valid position\n",
    "    starting_idxs: torch.tensor = torch.randint(0, len(data)-context_len, size=(batch_size,))\n",
    "    \n",
    "    #create a list of tensors, which we then stack\n",
    "    X=torch.stack([data[start:start+context_len] for start in starting_idxs])\n",
    "    y=torch.stack([data[start+1: start+context_len+1] for start in starting_idxs])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[44, 59, 52, 42, 39, 51, 43, 52],\n",
      "        [52, 43,  5, 43, 56,  1, 57, 53],\n",
      "        [ 0, 35, 43, 50, 50, 11,  1, 58],\n",
      "        [53, 58, 46, 43, 56,  6,  1, 40]])\n",
      "tensor([[59, 52, 42, 39, 51, 43, 52, 58],\n",
      "        [43,  5, 43, 56,  1, 57, 53,  1],\n",
      "        [35, 43, 50, 50, 11,  1, 58, 46],\n",
      "        [58, 46, 43, 56,  6,  1, 40, 59]])\n",
      "Total of 32 examples\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(196)\n",
    "\n",
    "X, y = create_batch(\"train\")\n",
    "print(X)\n",
    "print(y)\n",
    "print(f\"Total of {y.shape[0]*y.shape[1]} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Next Example in the Batch:\n",
      "\n",
      "Context: tensor([44]) --> Target: 59\n",
      "Context: tensor([44, 59]) --> Target: 52\n",
      "Context: tensor([44, 59, 52]) --> Target: 42\n",
      "Context: tensor([44, 59, 52, 42]) --> Target: 39\n",
      "Context: tensor([44, 59, 52, 42, 39]) --> Target: 51\n",
      "Context: tensor([44, 59, 52, 42, 39, 51]) --> Target: 43\n",
      "Context: tensor([44, 59, 52, 42, 39, 51, 43]) --> Target: 52\n",
      "Context: tensor([44, 59, 52, 42, 39, 51, 43, 52]) --> Target: 58\n",
      "\n",
      "Next Example in the Batch:\n",
      "\n",
      "Context: tensor([52]) --> Target: 43\n",
      "Context: tensor([52, 43]) --> Target: 5\n",
      "Context: tensor([52, 43,  5]) --> Target: 43\n",
      "Context: tensor([52, 43,  5, 43]) --> Target: 56\n",
      "Context: tensor([52, 43,  5, 43, 56]) --> Target: 1\n",
      "Context: tensor([52, 43,  5, 43, 56,  1]) --> Target: 57\n",
      "Context: tensor([52, 43,  5, 43, 56,  1, 57]) --> Target: 53\n",
      "Context: tensor([52, 43,  5, 43, 56,  1, 57, 53]) --> Target: 1\n",
      "\n",
      "Next Example in the Batch:\n",
      "\n",
      "Context: tensor([0]) --> Target: 35\n",
      "Context: tensor([ 0, 35]) --> Target: 43\n",
      "Context: tensor([ 0, 35, 43]) --> Target: 50\n",
      "Context: tensor([ 0, 35, 43, 50]) --> Target: 50\n",
      "Context: tensor([ 0, 35, 43, 50, 50]) --> Target: 11\n",
      "Context: tensor([ 0, 35, 43, 50, 50, 11]) --> Target: 1\n",
      "Context: tensor([ 0, 35, 43, 50, 50, 11,  1]) --> Target: 58\n",
      "Context: tensor([ 0, 35, 43, 50, 50, 11,  1, 58]) --> Target: 46\n",
      "\n",
      "Next Example in the Batch:\n",
      "\n",
      "Context: tensor([53]) --> Target: 58\n",
      "Context: tensor([53, 58]) --> Target: 46\n",
      "Context: tensor([53, 58, 46]) --> Target: 43\n",
      "Context: tensor([53, 58, 46, 43]) --> Target: 56\n",
      "Context: tensor([53, 58, 46, 43, 56]) --> Target: 6\n",
      "Context: tensor([53, 58, 46, 43, 56,  6]) --> Target: 1\n",
      "Context: tensor([53, 58, 46, 43, 56,  6,  1]) --> Target: 40\n",
      "Context: tensor([53, 58, 46, 43, 56,  6,  1, 40]) --> Target: 59\n"
     ]
    }
   ],
   "source": [
    "#Spelling out examples\n",
    "\n",
    "\n",
    "for batch in range(batch_size):\n",
    "    print(\"\\nNext Example in the Batch:\\n\")\n",
    "    for example_idx in range(context_len):\n",
    "        \n",
    "        context = X[batch][:example_idx+1] # Remember indexing is up to but not including e.ge [:0] is empty so [:0+1] is 1st example\n",
    "        target = y[batch][example_idx]\n",
    "        \n",
    "        print(f\"Context: {context} --> Target: {target}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Bigram Model\n",
    "\n",
    "As a baseline we will use the Bigram Language model. Which predicts the next character given a previous character. The prediction is made based on the frequency of bigram pairs (i.e. How many times does a come after r in our dataset). We collect all the pairwise combinations of our vocab (creating a (vocab_size, vocab_size) Look-up Table). We normalise over the row, to create a probability dist of the next character given a certain character. i.e. if the first character in our vocab is a. then the first row of our look-up table represents the frequency of every character in our vocab coming after a (i.e. given a). We normalise to create a probility distribution of the next character given a. These are the logits of the bigram model.\n",
    "\n",
    "On inference we can feed prob_dist into multinomial dist (which will sample the next character idx based on the prob_dist we gave). This idx is then used to index into our loo-up table, grabing the prob_dist of that character... and so on untill we reach an end character!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\"\n",
    "        Define Layers and Parameters\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bigram_lookup_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, idx, targets):\n",
    "        \"\"\"\n",
    "        Define how data flows through the layers of the network\n",
    "        \n",
    "        idx | (batch_dim, context_len): batches of indices of characters in our Vocab. \n",
    "        targets | (batch_dim, context_len): batch of target indices\n",
    "        \"\"\"\n",
    "        # For each idx you'll have a 1D array of length vocab_size\n",
    "        # shape idx (batch_dim, context_len) then prob_dists has shape (batch_dim, context_len, vocab_size)\n",
    "        prob_dists = self.bigram_lookup_table(idx)\n",
    "        \n",
    "        ## ----- Loss ----- ##\n",
    "        # ve- log likelihood <==> Cross entropy in this senario\n",
    "        # Cross Entropy wants the last dim to be num_classes (i.e. len(vocab))\n",
    "        # reshape/view\n",
    "        batch_size, context_len, num_classes = prob_dists.shape\n",
    "        prob_dists = prob_dists.view(batch_size*context_len, num_classes)\n",
    "        targets = targets.view(batch_size*context_len)\n",
    "        loss = F.cross_entropy(prob_dists, targets)\n",
    "        \n",
    "        return prob_dists, loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = BigramLanguageModel(vocab_size=len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5079,  0.5557, -1.7364,  ...,  0.3344,  0.8252,  0.2944],\n",
      "        [ 1.5031,  1.3577, -0.9287,  ...,  0.0875, -0.4830,  0.3045],\n",
      "        [-0.3904, -0.4811, -0.5190,  ..., -2.3388,  0.6987,  0.7192],\n",
      "        ...,\n",
      "        [ 0.2524, -0.5700,  0.7439,  ..., -2.2149,  1.0418,  0.0424],\n",
      "        [ 1.0648,  0.9654,  1.3579,  ..., -0.1666,  1.0698,  1.9603],\n",
      "        [-0.9576, -0.1427, -0.0548,  ...,  0.8169, -0.6708,  0.9897]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor(4.4722, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Randomly initalised Look-up table (we havent done any training yet)\n",
    "# Select the very first row, which should act as a prob dist \n",
    "prob_dist, loss= bigram(idx=X, targets=y)\n",
    "print(prob_dist)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
